{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bbb1491585a723",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5f066ceee1e2db",
   "metadata": {},
   "source": [
    "## Optional Jupyter Notebooks â€“ Task 2\n",
    "\n",
    "Be sure to read the [Introduction to Notebooks](IntroductiontoNotebooks.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5261f876bf7fb6",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "To begin, run the cell below (click inside the cell and press `Ctrl-Enter`) to load the framework necessary for Task 2. While you may import other libraries for experimentation, remember that only the standard Python libraries are supported by the Gradescope autograder.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9b2605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T17:32:23.685093Z",
     "start_time": "2024-08-09T17:32:23.664558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ybsha\\AppData\\Local\\Temp\\ipykernel_30920\\1391440526.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.decomposition\n",
    "import sklearn.model_selection\n",
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "task_path = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"src\"))\n",
    "sys.path.append(task_path)\n",
    "from task2 import *\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"tests\"))\n",
    "sys.path.append(utils_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2edea",
   "metadata": {},
   "source": [
    "## `tts`\n",
    "In this function, you will take a dataset, the name of its label column, a percentage of the data to put into the test set, whether you should stratify on the label column, and a random state to set the sklearn function. You will return features and labels for the training and test sets.\n",
    "\n",
    "At a high level, you can separate the task into two subtasks. The first is splitting your dataset into both features and labels (by columns), and the second is splitting your dataset into training and test sets (by rows). You should use the sklearn `train_test_split` function but will have to write wrapper code around it based on the input values we give you.\n",
    "##### Useful Resources\n",
    "* <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>\n",
    "* <https://developers.google.com/machine-learning/crash-course/framing/ml-terminology>\n",
    "* <https://stackoverflow.com/questions/40898019/what-is-the-difference-between-a-feature-and-a-label>\n",
    "\n",
    "##### INPUTS\n",
    "* `dataset` - a pandas DataFrame that contains some data\n",
    "* `label_col` - a string containing the name of the column that contains the `label` values (what our model wants to predict)\n",
    "* `test_size` - a float containing the decimal value of the percentage of the number of rows that the test set should be out of the dataset\n",
    "* `stratify` - a boolean (`True` or `False`) value indicating if the resulting train/test split should be stratified or not\n",
    "* `random_state` - an integer value to set the randomness of the function (useful for repeatability especially when autograding)\n",
    "\n",
    "##### OUTPUTS \n",
    "* `train_features` - a pandas DataFrame that contains the train rows and the feature columns\n",
    "* `test_features` - a pandas DataFrame that contains the test rows and the feature columns\n",
    "* `train_labels` - a pandas DataFrame that contains the train rows and the label column\n",
    "* `test_labels` - a pandas DataFrame that contains the test rows and the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7164b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "def tts(  dataset: pd.DataFrame,\n",
    "                       label_col: str, \n",
    "                       test_size: float,\n",
    "                       stratify: bool,\n",
    "                       random_state: int) -> tuple[pd.DataFrame,pd.DataFrame,pd.Series,pd.Series]:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = sklearn.model_selection.train_test_split(\n",
    "        dataset.drop(columns=[label_col]),\n",
    "        dataset[label_col],\n",
    "        test_size=test_size,\n",
    "        stratify=dataset[label_col],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_features,test_features,train_labels,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b577296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING TEST FOR Train Features DF\n",
      "\n",
      "Passed...\n",
      "\n",
      "RUNNING TEST FOR Test Features DF\n",
      "\n",
      "Passed...\n",
      "\n",
      "RUNNING TEST FOR Train Targets Series\n",
      "\n",
      "Passed...\n",
      "\n",
      "RUNNING TEST FOR Test Targets Series\n",
      "\n",
      "Passed...\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your code\n",
    "dataset = pd.read_csv(os.path.join(os.getcwd(),\"..\",\"task1\",\"sample.csv\"))\n",
    "#print(dataset)\n",
    "pkl_files_folder = os.path.join(os.getcwd(),\"..\",\"task2\",\"pkl_files\")\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_train_targets = pd.read_pickle(os.path.join(pkl_files_folder,\"train_targets_tts.pkl\"))\n",
    "#print(ans_train_targets)\n",
    "ans_test_targets = pd.read_pickle(os.path.join(pkl_files_folder,\"test_targets_tts.pkl\"))\n",
    "#print(ans_test_targets)\n",
    "\n",
    "target_col = \"target\"\n",
    "train_features,test_features,train_targets,test_targets = tts(dataset,target_col,test_size=.2,stratify=True,random_state=0)\n",
    "#print(train_features)\n",
    "#print(test_features)\n",
    "#print(train_targets)\n",
    "#print(test_targets)\n",
    "         \n",
    "if compare_submission_to_answer_df(train_features,ans_train_features,\"Train Features DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')        \n",
    "\n",
    "if compare_submission_to_answer_df(test_features,ans_test_features,\"Test Features DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')    \n",
    "\n",
    "if compare_submission_to_answer_series(train_targets,ans_train_targets,\"Train Targets Series\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')\n",
    "\n",
    "if compare_submission_to_answer_series(test_targets,ans_test_targets,\"Test Targets Series\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c6cb8",
   "metadata": {},
   "source": [
    "## `one_hot_encode_columns_train` and `one_hot_encode_columns_test`\n",
    "One Hot Encoding is the process of taking a column and returning a binary vector representing the various values within it. There is a separate function for the training and test datasets since they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them). \n",
    "\n",
    "### Pseudocode\n",
    "`one_hot_encode_columns_train()`\n",
    "0. In the `__init__()` method initialize an instance variable containing an sklearn `OneHotEncoder` with any Parameters you may need.\n",
    "1. Split `train_features` into into two DataFrames: one with only the columns you want to one hot encode (using `one_hot_encode_cols`) and another with all the other columns.\n",
    "2. Fit the `OneHotEncoder` using the DataFrame you split from `train_features` with the columns you want to encode.\n",
    "3. Transform the DataFrame you split from `train_features` with the columns you want to encode using the fitted `OneHotEncoder`.\n",
    "4. Create a DataFrame from the 2D array of data that the output from step 3 gave you, with column names in the form of `columnName_categoryName` (there should be an attribute in `OneHotEncoder` that can help you with this) and the same index that `train_features` had.\n",
    "5. Join the DataFrame you made in step 4 with the DataFrame of other columns from step 1.\n",
    "\n",
    "`one_hot_encode_columns_test()`\n",
    "1. Split `test_features` into two DataFrames: one with only the columns you want to one hot encode (using`one_hot_encode_cols`) and another with all the other columns.\n",
    "2. Transform the DataFrame you split from `train_features` with the columns you want to encode using the `OneHotEncoder` you fit in `one_hot_encode_columns_train()`\n",
    "3. Create a DataFrame from the 2D array of data that the output from step 2 gave you, with column names in the form of `columnName_categoryName` (there should be an attribute in `OneHotEncoder` that can help you with this) and the same index that `test_features` had.\n",
    "4. Join the DataFrame you made in step 3 with the DataFrame of other columns from step 1.\n",
    "\n",
    "### Example Walkthrough (from Local Testing suite):\n",
    "\n",
    "<!--\n",
    "| Item         | Price     | Count      | Type       |\n",
    "|--------------|-----------|------------|------------|\n",
    "| Apples       | 1.99      | 7          | Fruit      |\n",
    "| Broccoli     | 1.29      | 435        | Vegetable   |\n",
    "| Bananas      | 0.99      | 123        | Fruit      |\n",
    "| Oranges      | 2.79      | 25         | Fruit      |\n",
    "| Pineapples   | 4.89      | 5234       | Fruit      |\n",
    "-->\n",
    "#### INPUTS:\n",
    "\n",
    "##### one_hot_encode_cols \n",
    " `[\"color\",\"version\"]`\n",
    "\n",
    "##### Train Features\n",
    "<html>\n",
    "<style>\n",
    "table, th, td {\n",
    "  text-align: Center;\n",
    "}\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr><th width=100px>index</th><th width=100px>color</th><th width=100px>version</th><th width=100px>cost</th><th width=100px>height</th></tr>\n",
    "  <tr><td>0</td><td>red</td><td>1</td><td>5.99</td><td>12</td> </tr>\n",
    "  <tr><td>6</td><td>yellow</td><td>6</td><td>10.99</td><td>18</td></tr>\n",
    "  <tr><td>3</td><td>red</td><td>1</td><td>5.99</td><td>15</td></tr>\n",
    "  <tr><td>9</td><td>red</td><td>8</td><td>12.99</td><td>21</td></tr>\n",
    "  <tr><td>2</td><td>blue</td><td>3</td><td>5.99</td><td>14</td> </tr>\n",
    "  <tr><td>5</td><td>orange</td><td>5</td><td>10.99</td><td>17</td> </tr>\n",
    "  <tr><td>1</td><td>green</td><td>2</td><td>5.99</td><td>13</td> </tr>\n",
    "  <tr><td>7</td><td>green</td><td>2</td><td>12.99</td><td>19</td> </tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "##### Test Features\n",
    "<html>\n",
    "<style>\n",
    "table, th, td {\n",
    "  text-align: Center;\n",
    "}\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr><th width=100px>index</th><th width=100px>color</th><th width=100px>version</th><th width=100px>cost</th><th width=100px>height</th></tr>\n",
    "  <tr><td>4</td><td>purple</td><td>4</td><td>10.99</td><td>16</td> </tr>\n",
    "  <tr><td>8</td><td>blue</td><td>3</td><td>12.99</td><td>20</td></tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "239d2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def one_hot_encode_columns_train(train_features,test_features,one_hot_encode_cols) -> pd.DataFrame:\n",
    "    \n",
    "    df_encode_cols = train_features[one_hot_encode_cols]\n",
    "    df_other_cols = train_features.drop(columns=one_hot_encode_cols)\n",
    "    \n",
    "    encoder = sklearn.preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "    encoded = encoder.fit_transform(df_encode_cols)\n",
    "    features = encoder.get_feature_names_out(one_hot_encode_cols)\n",
    "\n",
    "    df_encoded = pd.DataFrame(encoded, columns=features, index=train_features.index)\n",
    "    df_concat = pd.concat([df_encoded, df_other_cols], axis=1)\n",
    "\n",
    "    return df_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3471e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded array shape: (8, 11)\n",
      "Number of feature names: 11\n",
      "Result DataFrame shape: (8, 11)\n",
      "Result DataFrame shape: (8, 13)\n",
      "\n",
      "RUNNING TEST FOR One Hot Encoded Train DF\n",
      "\n",
      "Passed...\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your code\n",
    "def double_height(dataframe:pd.DataFrame):\n",
    "    return dataframe[\"height\"] * 2\n",
    "\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_train_features_ohe = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_ohe.pkl\"))\n",
    "#print(ans_train_features_ohe)\n",
    "\n",
    "one_hot_encode_cols = [\"color\",\"version\"]\n",
    "train_features_ohe = one_hot_encode_columns_train(ans_train_features,ans_test_features,one_hot_encode_cols)\n",
    "#print(train_features_ohe)\n",
    "\n",
    "if compare_submission_to_answer_df(train_features_ohe,ans_train_features_ohe,\"One Hot Encoded Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c8fab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here    \n",
    "def one_hot_encode_columns_test(train_features,test_features,one_hot_encode_cols) -> pd.DataFrame:\n",
    "    df_encode_cols = test_features[one_hot_encode_cols]\n",
    "    df_other_cols = test_features.drop(columns=one_hot_encode_cols)\n",
    "    \n",
    "    encoder = sklearn.preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    \n",
    "    encoded = encoder.fit_transform(df_encode_cols)\n",
    "    features = encoder.get_feature_names_out(one_hot_encode_cols)\n",
    "\n",
    "    df_encoded = pd.DataFrame(encoded, columns=features, index=test_features.index)\n",
    "    df_concat = pd.concat([df_encoded, df_other_cols], axis=1)\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4edc1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING TEST FOR One Hot Encoded Test DF\n",
      "\n",
      "You do not have the correct number of columns in One Hot Encoded Test DF\n",
      "\n",
      "Failed...\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your code\n",
    "def double_height(dataframe:pd.DataFrame):\n",
    "    return dataframe[\"height\"] * 2\n",
    "\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_test_features_ohe = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_ohe.pkl\"))\n",
    "#print(ans_test_features_ohe)\n",
    "\n",
    "one_hot_encode_cols = [\"color\",\"version\"]\n",
    "test_features_ohe = one_hot_encode_columns_test(ans_train_features,ans_test_features,one_hot_encode_cols)\n",
    "#print(test_features_ohe)\n",
    "\n",
    "if compare_submission_to_answer_df(test_features_ohe,ans_test_features_ohe,\"One Hot Encoded Test DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbdc0e",
   "metadata": {},
   "source": [
    "## `min_max_scaled_columns_train` and `min_max_scaled_columns_test`\n",
    "Min/Max Scaling is a process of scaling ints/floats from a min and max value in a series to between 0 and 1. The function for how scikit-learn does this is shown below, but for this assignment, you should just use the linked scikit-learn function.\n",
    "```python\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "```\n",
    "There is a separate function for the training and test datasets because they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them).\n",
    "\n",
    "Example Dataframe:\n",
    "<!--\n",
    "| Item         | Price     | Count      | Type       |\n",
    "|--------------|-----------|------------|------------|\n",
    "| Apples       | 1.99      | 7          | Fruit      |\n",
    "| Broccoli     | 1.29      | 435        | Vegtable   |\n",
    "| Bananas      | 0.99      | 123        | Fruit      |\n",
    "| Oranges      | 2.79      | 25         | Fruit      |\n",
    "| Pineapples   | 4.89      | 5234       | Fruit      |\n",
    "-->\n",
    "\n",
    "<html>\n",
    "<style>\n",
    "table, th, td {\n",
    "  text-align: Center;\n",
    "}\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr><th width=100px>Item</th><th width=100px>Price</th><th width=100px>Count</th><th width=100px>Type</th></tr>\n",
    "  <tr><td>Apples</td><td>1.99</td><td>7</td><td>Fruit</td> </tr>\n",
    "  <tr><td>Broccoli</td><td>1.29</td><td>435</td><td>Vegtable</td></tr>\n",
    "  <tr><td>Bananas</td><td>0.99</td><td>123</td><td>Fruit</td></tr>\n",
    "  <tr><td>Oranges</td><td>2.79</td><td>25</td><td>Fruit</td></tr>\n",
    "  <tr><td>Pineapples</td><td>4.89</td><td>5234</td><td>Fruit</td> </tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "Example One Hot Encoded Dataframe (rounded to 4 decimal places):\n",
    "<!--\n",
    "| Item         | Price     | Count      | Type       |\n",
    "|--------------|-----------|------------|------------|\n",
    "| Apples       | 0.2564    | 7          | Fruit      |\n",
    "| Broccoli     | 0.0769    | 435        | Vegtable   |\n",
    "| Bananas      | 0         | 123        | Fruit      |\n",
    "| Oranges      | 0.4615    | 25         | Fruit      |\n",
    "| Pineapples   | 1         | 5234       | Fruit      |\n",
    "-->\n",
    "\n",
    "<html>\n",
    "<style>\n",
    "table, th, td {\n",
    "  text-align: Center;\n",
    "}\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr><th width=100px>Item</th><th width=100px>Price</th><th width=100px>Count</th><th width=100px>Type</th></tr>\n",
    "  <tr><td>Apples</td><td>0.2564</td><td>7</td><td>Fruit</td> </tr>\n",
    "  <tr><td>Broccoli</td><td>0.0769</td><td>435</td><td>Vegtable</td></tr>\n",
    "  <tr><td>Bananas</td><td>0</td><td>123</td><td>Fruit</td></tr>\n",
    "  <tr><td>Oranges</td><td>0.4615</td><td>25</td><td>Fruit</td></tr>\n",
    "  <tr><td>Pineapples</td><td>1</td><td>5234</td><td>Fruit</td> </tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def min_max_scaled_columns_train(train_features,test_features,min_max_scale_cols) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    min_max_scaled_dataset = pd.DataFrame()\n",
    "\n",
    "    return min_max_scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_train_features_mms = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_mms.pkl\"))\n",
    "#print(ans_train_features_mms)\n",
    "\n",
    "min_max_scale_cols = [\"cost\"]\n",
    "train_features_mms = min_max_scaled_columns_train(ans_train_features,ans_test_features,min_max_scale_cols)\n",
    "#print(train_features_mms)\n",
    "\n",
    "if compare_submission_to_answer_df(train_features_mms,ans_train_features_mms,\"Min Max Scaled Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here    \n",
    "def min_max_scaled_columns_test(train_features,test_features,min_max_scale_cols) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    min_max_scaled_dataset = pd.DataFrame()\n",
    "    \n",
    "    return min_max_scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_test_features_mms = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_mms.pkl\"))\n",
    "#print(ans_test_features_mms)\n",
    "\n",
    "min_max_scale_cols = [\"cost\"]\n",
    "test_features_mms = min_max_scaled_columns_test(ans_train_features,ans_test_features,min_max_scale_cols)\n",
    "#print(test_features_mms)\n",
    "\n",
    "if compare_submission_to_answer_df(test_features_mms,ans_test_features_mms,\"Min Max Scaled Test DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dce65",
   "metadata": {},
   "source": [
    "## `pca_train` and `pca_test`\n",
    "Principal Component Analysis is a dimensionality reduction technique (column reduction). It aims to take the variance in your input columns and map the columns into N columns that contain as much of the variance as it can. This technique can be useful if you are trying to train a model faster and has some more advanced uses, especially when training models on data which has many columns but few rows. There is a separate function for the training and test datasets because they should be handled separately to avoid data leakage (see the 3rd link in Useful Resources for a little more info on how to handle them).\n",
    "\n",
    "**Note:** For the Autograder, use the column naming scheme of column names: component_1, component_2 .. component_n for the `n_components` passed into the `__init__` method.\n",
    "\n",
    "**Note 2:** For your PCA outputs to match the Autograder, make sure you set the seed using a random state of 0 when you initialize the PCA function.\n",
    "\n",
    "**Note 3:** Since PCA does not work with NA values, make sure you drop any columns that have NA values before running PCA.\n",
    " \n",
    "##### Useful Resources\n",
    "* <https://builtin.com/data-science/step-step-explanation-principal-component-analysis>\n",
    "* <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA>\n",
    "* <https://datascience.stackexchange.com/questions/103211/do-we-need-to-pre-process-both-the-test-and-train-data-set>\n",
    "\n",
    "##### INPUTS\n",
    "Use the needed instance variables you set in the `__init__` method\n",
    "\n",
    "##### OUTPUTS \n",
    "a pandas DataFrame with the generated pca values and using column names: component_1, component_2 .. component_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830db93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def pca_train(train_features,test_features,n_components) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    pca_dataset = pd.DataFrame()\n",
    "    \n",
    "    return pca_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\")).drop(columns=[\"color\"])\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\")).drop(columns=[\"color\"])\n",
    "#print(ans_test_features)\n",
    "ans_train_features_pca = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_pca.pkl\"))\n",
    "#print(ans_train_features_pca)\n",
    "\n",
    "n_components = 2\n",
    "train_features_pca = pca_train(ans_train_features,ans_test_features,n_components)\n",
    "#print(train_features_pca)\n",
    "\n",
    "if compare_submission_to_answer_df(train_features_pca.round(4),ans_train_features_pca.round(4),\"PCA Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here    \n",
    "def pca_test(train_features,test_features,n_components) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    pca_dataset = pd.DataFrame()\n",
    "        \n",
    "    return pca_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c508690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\")).drop(columns=[\"color\"])\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\")).drop(columns=[\"color\"])\n",
    "#print(ans_test_features)\n",
    "ans_test_features_pca = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_pca.pkl\"))\n",
    "#print(ans_test_features_pca)\n",
    "\n",
    "n_components = 2\n",
    "test_features_pca = pca_test(ans_train_features,ans_test_features,n_components)\n",
    "#print(test_features_pca)\n",
    "\n",
    "if compare_submission_to_answer_df(test_features_pca.round(4),ans_test_features_pca.round(4),\"PCA Test DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a52e8",
   "metadata": {},
   "source": [
    "## `feature_engineering_train`, `feature_engineering_test`\n",
    "Feature Engineering is a process of using domain knowledge (physics, geometry, sports statistics, business metrics, etc.) to create new features (columns) out of the existing data. This could mean creating an area feature when given the length and width of a triangle or extracting the major and minor version number from a software version or more complex logic depending on the scenario. For this method, you will be taking in a dictionary with a column name and a function (that takes in a DataFrame and returns a column) and using that to create a new column with the name in the dictionary key.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "def double_height(dataframe:pd.DataFrame):\n",
    "    return dataframe[\"height\"] * 2\n",
    "def half_height(dataframe:pd.DataFrame):\n",
    "    return dataframe[\"height\"] / 2\n",
    "feature_engineering_functions = {\"double_height\":double_height,\"half_height\":half_height}\n",
    "```\n",
    "With the above functions, you would create two new columns named \"double_height\" and \"half_height\".\n",
    "\n",
    "##### Useful Resources\n",
    "* <https://en.wikipedia.org/wiki/Feature_engineering>\n",
    "* <https://www.geeksforgeeks.org/what-is-feature-engineering/>\n",
    "\n",
    "##### INPUTS\n",
    "Use the needed instance variables you set in the `__init__` method\n",
    "\n",
    "##### OUTPUTS \n",
    "a pandas dataframe with the features described in `feature_engineering_train` and `feature_engineering_test` added as new columns and all other columns in the dataframe unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def feature_engineering_train(train_features,test_features,feature_engineering_functions) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    feature_engineered_dataset = pd.DataFrame()\n",
    "\n",
    "    return feature_engineered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00500635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_train_features_fe = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_fe.pkl\"))\n",
    "#print(ans_train_features_fe)\n",
    "\n",
    "feature_engineering_functions = {\"double_height\":double_height}\n",
    "train_features_fe = feature_engineering_train(ans_train_features,ans_test_features,feature_engineering_functions)\n",
    "#print(train_features_fe)\n",
    "\n",
    "if compare_submission_to_answer_df(train_features_fe,ans_train_features_fe,\"Feature Engineered Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f58f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def feature_engineering_test(train_features,test_features,feature_engineering_functions) -> pd.DataFrame:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    feature_engineered_dataset = pd.DataFrame()\n",
    "\n",
    "    return feature_engineered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3168bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_test_features_fe = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_fe.pkl\"))\n",
    "#print(ans_test_features_fe)\n",
    "\n",
    "feature_engineering_functions = {\"double_height\":double_height}\n",
    "test_features_fe = feature_engineering_test(ans_train_features,ans_test_features,feature_engineering_functions)\n",
    "#print(test_features_fe)\n",
    "\n",
    "if compare_submission_to_answer_df(test_features_fe,ans_test_features_fe,\"Feature Engineered Test DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d867e",
   "metadata": {},
   "source": [
    "## PreprocessDataset:`preprocess_train`, `preprocess_test`\n",
    "Now, we will put three of the above methods together into a preprocess function. This function will take in a dataset and perform **encoding**, **scaling**, and **feature engineering** using the above methods and their respective columns.\n",
    "\n",
    "##### Useful Resources\n",
    "See resources for one hot encoding, min/max scaling and feature engineering above\n",
    "\n",
    "##### INPUTS\n",
    "Use the needed instance variables you set in the `__init__` method\n",
    "\n",
    "##### OUTPUTS \n",
    "a pandas dataframe for both test and train features with the columns in `one_hot_encode_cols` encoded, the columns in `min_max_scale_cols` scaled and the columns described in `feature_engineering_functions` engineered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def preprocess_train(train_features,test_features,one_hot_encode_cols,min_max_scale_cols,feature_engineering_functions) -> tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    train_features = pd.DataFrame()\n",
    "    \n",
    "    return train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a45303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_train_features_preprocess = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_preprocess.pkl\"))\n",
    "#print(ans_train_features_preprocess)\n",
    "\n",
    "one_hot_encode_cols = [\"color\",\"version\"]\n",
    "min_max_scale_cols = [\"cost\"]\n",
    "n_components = 2\n",
    "feature_engineering_functions = {\"double_height\":double_height}\n",
    "\n",
    "train_features_preprocess = preprocess_train(ans_train_features,ans_test_features,one_hot_encode_cols,min_max_scale_cols,feature_engineering_functions)\n",
    "#print(train_features_preprocess)\n",
    "\n",
    "if compare_submission_to_answer_df(train_features_preprocess,ans_train_features_preprocess,\"Preprocessed Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed874383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def preprocess_test(train_features,test_features,one_hot_encode_cols,min_max_scale_cols,feature_engineering_functions) -> tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task2.html and implement the function as described\n",
    "    test_features = pd.DataFrame()\n",
    "    \n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98510022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "# Run this cell to test your code\n",
    "ans_train_features = pd.read_pickle(os.path.join(pkl_files_folder,\"train_feats_tts.pkl\"))\n",
    "#print(ans_train_features)\n",
    "ans_test_features = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_tts.pkl\"))\n",
    "#print(ans_test_features)\n",
    "ans_test_features_preprocess = pd.read_pickle(os.path.join(pkl_files_folder,\"test_feats_preprocess.pkl\"))\n",
    "#print(ans_test_features_preprocess)\n",
    "\n",
    "one_hot_encode_cols = [\"color\",\"version\"]\n",
    "min_max_scale_cols = [\"cost\"]\n",
    "n_components = 2\n",
    "feature_engineering_functions = {\"double_height\":double_height}\n",
    "\n",
    "test_features_preprocess = preprocess_test(ans_train_features,ans_test_features,one_hot_encode_cols,min_max_scale_cols,feature_engineering_functions)\n",
    "#print(train_features_preprocess)\n",
    "\n",
    "if compare_submission_to_answer_df(test_features_preprocess,ans_test_features_preprocess,\"Preprocessed Train DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35555414",
   "metadata": {},
   "source": [
    "# You have successfully reached the end of this notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6035_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
