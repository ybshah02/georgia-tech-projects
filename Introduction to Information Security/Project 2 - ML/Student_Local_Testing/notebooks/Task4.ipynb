{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster\n",
    "import yellowbrick.cluster\n",
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "task_path = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "sys.path.append(task_path)\n",
    "from src.task4 import *\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(),\"..\",\"tests\"))\n",
    "sys.path.append(utils_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all input and output datasets. These will be used in the validation step\n",
    "train_features = pd.read_csv(os.path.join(os.getcwd(),\"..\",\"task4\",\"wine_train_features.csv\"), index_col=0)\n",
    "#print('train_features : \\n',train_features)\n",
    "test_features  = pd.read_csv(os.path.join(os.getcwd(),\"..\",\"task4\",\"wine_test_features.csv\"), index_col=0)\n",
    "#print('test_features : \\n',test_features)\n",
    "train_targets  = pd.read_csv(os.path.join(os.getcwd(),\"..\",\"task4\",\"wine_train_targets.csv\"), index_col=0)\n",
    "#print('train_targets : \\n',train_targets)\n",
    "test_targets   = pd.read_csv(os.path.join(os.getcwd(),\"..\",\"task4\",\"wine_test_targets.csv\"), index_col=0)\n",
    "#print('test_targets : \\n',test_targets)\n",
    "logreg_importance_df_ans = pd.read_pickle(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"wine_logreg_importance.pkl\"))\n",
    "#print('logreg_importance_df_ans : \\n',importance_df_ans)\n",
    "rf_importance_df_ans = pd.read_pickle(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"wine_rf_importance.pkl\"))\n",
    "#print('rf_importance_df_ans : \\n',importance_df_ans)\n",
    "gb_importance_df_ans = pd.read_pickle(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"wine_gb_importance.pkl\"))\n",
    "#print('gb_importance_df_ans : \\n',importance_df_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8047f",
   "metadata": {},
   "source": [
    "## `calculate_naive_metrics`\n",
    "\n",
    "A Naive model is a very simple model/prediction that can help to frame how well any more sophisticated model is doing. Since the Naive Model is incredibly basic (often a constant result or a randomly selected result), we should expect that any more sophisticated model that we train should outperform it. If the Naive Model beats a trained model, it can mean that additional data (rows or columns) is needed in the dataset to improve the model or that the dataset doesn't have a strong enough signal for the target we want to predict. \n",
    "In this function you will use the approach of a constant output Naive model. You will calculate 4 metrics (`accuracy`,`recall`,`precision` and `fscore`) for the training and test datasets for a given constant integer as your prediction (passed into the function as the variable `naive_assumption`). \n",
    "\n",
    "\n",
    "##### Useful Resources\n",
    "* https://machinelearningmastery.com/how-to-develop-and-evaluate-naive-classifier-strategies-using-probability/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "##### INPUTS\n",
    "* `train_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `train_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `naive_assumption` - an integer that should be used as the result from the Naive model\n",
    "\n",
    "##### OUTPUTS \n",
    "A completed `ModelMetrics` object with a training and test metrics dictionary with each one of the metrics **rounded to 4 decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5665963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_naive_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, naive_assumption:int) -> ModelMetrics:\n",
    "    # TODO: Write the necessary code to calculate accuracy, recall, precision and fscore given a train and test dataframe\n",
    "    # and a train and test target series and naive assumption \n",
    "\n",
    "    train_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0\n",
    "        }\n",
    "    test_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0\n",
    "        }\n",
    "    \n",
    "    naive_metrics = ModelMetrics(\"Naive\",train_metrics,test_metrics,None)\n",
    "    return naive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b19eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "naive_assumption = 1\n",
    "# Answers for Wine Dataset\n",
    "import pickle\n",
    "with open(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"naive_metrics.pkl\"), 'rb') as file:\n",
    "    naive_metrics_ans = pickle.load(file)\n",
    "train_accuracy_ans = naive_metrics_ans.train_metrics[\"accuracy\"]\n",
    "train_recall_ans = naive_metrics_ans.train_metrics[\"recall\"]\n",
    "train_precision_ans = naive_metrics_ans.train_metrics[\"precision\"]\n",
    "train_fscore_ans = naive_metrics_ans.train_metrics[\"fscore\"]\n",
    "test_accuracy_ans = naive_metrics_ans.test_metrics[\"accuracy\"]\n",
    "test_recall_ans = naive_metrics_ans.test_metrics[\"recall\"]\n",
    "test_precision_ans = naive_metrics_ans.test_metrics[\"precision\"]\n",
    "test_fscore_ans = naive_metrics_ans.test_metrics[\"fscore\"]\n",
    "# Calculate Metrics with Student's Function\n",
    "naive_metrics = calculate_naive_metrics(train_features, test_features, train_targets, test_targets, naive_assumption)\n",
    "\n",
    "train_accuracy = naive_metrics.train_metrics[\"accuracy\"]\n",
    "assert train_accuracy == train_accuracy_ans\n",
    "train_recall = naive_metrics.train_metrics[\"recall\"]\n",
    "assert train_recall == train_recall_ans\n",
    "train_precision = naive_metrics.train_metrics[\"precision\"]\n",
    "assert train_precision == train_precision_ans\n",
    "train_fscore = naive_metrics.train_metrics[\"fscore\"]\n",
    "assert train_fscore == train_fscore_ans\n",
    "test_accuracy = naive_metrics.test_metrics[\"accuracy\"]\n",
    "assert test_accuracy == test_accuracy_ans\n",
    "train_recall = naive_metrics.test_metrics[\"recall\"]\n",
    "assert train_recall == test_recall_ans\n",
    "train_precision = naive_metrics.test_metrics[\"precision\"]\n",
    "assert train_precision == test_precision_ans\n",
    "train_fscore = naive_metrics.test_metrics[\"fscore\"]\n",
    "assert train_fscore == test_fscore_ans\n",
    "\n",
    "print('Passed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe4029",
   "metadata": {},
   "source": [
    "## `calculate_logistic_regression_metrics`\n",
    "\n",
    "A Logistic Regression model is a simple and more explainable statistical model that can be used to estimate the probability of an event (log-odds). At a high level, a Logistic Regression model uses data in the training set to estimate a column's weight in a linear approximation function (conceptually this is similar to estimating `m` for each column in the line formula you probably know well: `y = m*x + b`). If you are interested in learning more, you can read up on the Math behind how this works. For this project, we are more focused on showing you how to apply these models, so you can just use the sklearn Logistic Regression model in your code. \n",
    "\n",
    "For this task use Sklearn's Logistic Regression class and complete the following subtasks:\n",
    "* Train a Logistic Regression model (initialized using the kwargs passed into the function)\n",
    "* Predict scores for training and test datasets and calculate the 7 metrics listed below for the training and test datasets using predictions from the fit model. (All rounded to 4 decimal places)\n",
    "  * `accuracy`\n",
    "  * `recall`\n",
    "  * `precision`\n",
    "  * `fscore`\n",
    "  * `false positive rate (fpr)`\n",
    "  * `false negative rate (fnr)`\n",
    "  * `Area Under the Curve of Receiver Operating Characteristics Curve (roc_auc)`\n",
    "* Use RFE to select the top 10 features \n",
    "* Train a model using these selected features (initialized using the kwargs passed into the function)\n",
    "* Create a Feature Importance DataFrame from the model trained on the top 10 features: \n",
    "  * Use the top 10 features sort by absolute value of the coefficient from biggest to smallest \n",
    "  * Make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [`Feature`] and imp_col [`Importance`] \n",
    "  * round the importances to 4 decimal places\n",
    "  * Reset the index to 0-9 you can do this the same way you did in task1 \n",
    "\n",
    "**NOTE:** Make sure you use the predicted probabilities for roc auc\n",
    "\n",
    "##### Useful Resources\n",
    "* https://stats.libretexts.org/Bookshelves/Introductory_Statistics/OpenIntro_Statistics_(Diez_et_al)./08%3A_Multiple_and_Logistic_Regression/8.04%3A_Introduction_to_Logistic_Regression\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "\n",
    "##### INPUTS\n",
    "* `train_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `train_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `logreg_kwargs` - a dictionary with keyword arguments that can be passed directly to the sklearn Logistic Regression class\n",
    "\n",
    "##### OUTPUTS \n",
    "* A completed `ModelMetrics` object with a training and test metrics dictionary with each one of the metrics **rounded to 4 decimal places**\n",
    "* An sklearn Logistic Regression model object fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_logistic_regression_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, logreg_kwargs) -> tuple[ModelMetrics,LogisticRegression]:\n",
    "    # TODO: Write the necessary code to train a logistic regression binary classifiaction model and calculate accuracy, recall, precision, fscore, \n",
    "    # false positive rate, false negative rate and area under the reciever operator curve given a train and test dataframe and train and test target series \n",
    "    # and keyword arguments for the logistic regrssion model\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    train_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "    test_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "\n",
    "    log_reg_importance = pd.DataFrame()\n",
    "    log_reg_metrics = ModelMetrics(\"Logistic Regression\",train_metrics,test_metrics,log_reg_importance)\n",
    "\n",
    "    return log_reg_metrics,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "logreg_kwargs = {'penalty':'l1','fit_intercept':False,'solver':'liblinear','random_state':0}\n",
    "# Answers for Wine Dataset\n",
    "with open(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"logreg_metrics.pkl\"), 'rb') as file:\n",
    "    logreg_metrics_ans = pickle.load(file)\n",
    "train_accuracy_ans = logreg_metrics_ans.train_metrics[\"accuracy\"]\n",
    "train_recall_ans = logreg_metrics_ans.train_metrics[\"recall\"]\n",
    "train_precision_ans = logreg_metrics_ans.train_metrics[\"precision\"]\n",
    "train_fscore_ans = logreg_metrics_ans.train_metrics[\"fscore\"]\n",
    "train_fpr_ans = logreg_metrics_ans.train_metrics[\"fpr\"]\n",
    "train_fnr_ans = logreg_metrics_ans.train_metrics[\"fnr\"]\n",
    "train_roc_auc_ans = logreg_metrics_ans.train_metrics[\"roc_auc\"]\n",
    "\n",
    "test_accuracy_ans = logreg_metrics_ans.test_metrics[\"accuracy\"]\n",
    "test_recall_ans = logreg_metrics_ans.test_metrics[\"recall\"]\n",
    "test_precision_ans = logreg_metrics_ans.test_metrics[\"precision\"]\n",
    "test_fscore_ans = logreg_metrics_ans.test_metrics[\"fscore\"]\n",
    "test_fpr_ans = logreg_metrics_ans.test_metrics[\"fpr\"]\n",
    "test_fnr_ans = logreg_metrics_ans.test_metrics[\"fnr\"]\n",
    "test_roc_auc_ans = logreg_metrics_ans.test_metrics[\"roc_auc\"]\n",
    "\n",
    "# Calculate Metrics with Student's Function\n",
    "logreg_metrics,_ = calculate_logistic_regression_metrics(train_features, test_features, train_targets, test_targets, logreg_kwargs)\n",
    "\n",
    "train_accuracy = logreg_metrics.train_metrics[\"accuracy\"]\n",
    "assert train_accuracy == train_accuracy_ans\n",
    "train_recall = logreg_metrics.train_metrics[\"recall\"]\n",
    "assert train_recall == train_recall_ans\n",
    "train_precision = logreg_metrics.train_metrics[\"precision\"]\n",
    "assert train_precision == train_precision_ans\n",
    "train_fscore = logreg_metrics.train_metrics[\"fscore\"]\n",
    "assert train_fscore == train_fscore_ans\n",
    "train_fpr = logreg_metrics.train_metrics[\"fpr\"]\n",
    "assert train_fpr == train_fpr_ans\n",
    "train_fnr = logreg_metrics.train_metrics[\"fnr\"]\n",
    "assert train_fnr == train_fnr_ans\n",
    "train_roc_auc = logreg_metrics.train_metrics[\"roc_auc\"]\n",
    "assert train_roc_auc == train_roc_auc_ans\n",
    "test_accuracy = logreg_metrics.test_metrics[\"accuracy\"]\n",
    "assert test_accuracy == test_accuracy_ans\n",
    "test_recall = logreg_metrics.test_metrics[\"recall\"]\n",
    "assert test_recall == test_recall_ans\n",
    "test_precision = logreg_metrics.test_metrics[\"precision\"]\n",
    "assert test_precision == test_precision_ans\n",
    "test_fscore = logreg_metrics.test_metrics[\"fscore\"]\n",
    "assert test_fscore == test_fscore_ans\n",
    "test_fpr = logreg_metrics.test_metrics[\"fpr\"]\n",
    "assert test_fpr == test_fpr_ans\n",
    "test_fnr = logreg_metrics.test_metrics[\"fnr\"]\n",
    "assert test_fnr == test_fnr_ans\n",
    "test_roc_auc = logreg_metrics.test_metrics[\"roc_auc\"]\n",
    "assert test_roc_auc == test_roc_auc_ans\n",
    "importance_df = logreg_metrics.feat_imp_df\n",
    "if compare_submission_to_answer_df(importance_df.round(4),logreg_importance_df_ans,\"LR Feature Importance DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e371e3",
   "metadata": {},
   "source": [
    "## `calculate_random_forest_metrics`\n",
    "\n",
    "A Random Forest model is a more complex model than the Naive and Logistic Regression Models you have trained so far. It can still be used to estimate the probability of an event, but achieves this using a different underlying structure: a Tree Based Model. Conceptually, this looks a lot like lots of if/else statements chained together into a \"tree\". A Random Forest Expands on this and trains many different trees with different subsets of the data and starting conditions to get a better estimate than a single tree would give. For this project, we are more focused on showing you how to apply these models, so you can just use the sklearn Random Forest model in your code. \n",
    "\n",
    "For this task use Sklearn's Random Forest Classifier class and complete the following subtasks:\n",
    "* Train a Random Forest model (initialized using the kwargs passed into the function)\n",
    "* Predict scores for training and test datasets and calculate the 7 metrics listed below for the training and test datasets using predictions from the fit model. (All rounded to 4 decimal places)\n",
    "  * `accuracy`\n",
    "  * `recall`\n",
    "  * `precision`\n",
    "  * `fscore`\n",
    "  * `false positive rate (fpr)`\n",
    "  * `false negative rate (fnr)`\n",
    "  * `Area Under the Curve of Receiver Operating Characteristics Curve (roc_auc)`\n",
    "* Create a Feature Importance DataFrame from the trained model: \n",
    "  * **Do Not Use RFE for feature selection**\n",
    "  * Use the top 10 features selected by the built in method (sorted from biggest to smallest) \n",
    "  * Make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [`Feature`] and imp_col [`Importance`] \n",
    "  * Round the importances to 4 decimal places\n",
    "  * Reset the index to 0-9 you can do this the same way you did in task1 \n",
    "\n",
    "**NOTE:** Make sure you use the predicted probabilities for roc auc\n",
    "\n",
    "##### Useful Resources\n",
    "* https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "\n",
    "##### INPUTS\n",
    "* `train_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `train_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `rf_kwargs` - a dictionary with keyword arguments that can be passed directly to the sklearn RandomForestClassifier class\n",
    "\n",
    "##### OUTPUTS \n",
    "* A completed `ModelMetrics` object with a training and test metrics dictionary with each one of the metrics **rounded to 4 decimal places**\n",
    "* An sklearn Random Forest model object fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2325937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "def calculate_random_forest_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, rf_kwargs) -> tuple[ModelMetrics,RandomForestClassifier]:\n",
    "    # TODO: Read the function description in https://github.gatech.edu/pages/cs6035-tools/cs6035-tools.github.io/Projects/Machine_Learning/Task4.html and implement the function as described\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    train_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "    test_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "\n",
    "    rf_importance = pd.DataFrame()\n",
    "    rf_metrics = ModelMetrics(\"Random Forest\",train_metrics,test_metrics,rf_importance)\n",
    "\n",
    "    return rf_metrics,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "rf_kwargs = {'n_estimators':150,'max_depth':3,'criterion':'log_loss','random_state':0}\n",
    "# Answers for Wine Dataset\n",
    "with open(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"rf_metrics.pkl\"), 'rb') as file:\n",
    "    rf_metrics_ans = pickle.load(file)\n",
    "train_accuracy_ans = rf_metrics_ans.train_metrics[\"accuracy\"]\n",
    "train_recall_ans = rf_metrics_ans.train_metrics[\"recall\"]\n",
    "train_precision_ans = rf_metrics_ans.train_metrics[\"precision\"]\n",
    "train_fscore_ans = rf_metrics_ans.train_metrics[\"fscore\"]\n",
    "train_fpr_ans = rf_metrics_ans.train_metrics[\"fpr\"]\n",
    "train_fnr_ans = rf_metrics_ans.train_metrics[\"fnr\"]\n",
    "train_roc_auc_ans = rf_metrics_ans.train_metrics[\"roc_auc\"]\n",
    "\n",
    "test_accuracy_ans = rf_metrics_ans.test_metrics[\"accuracy\"]\n",
    "test_recall_ans = rf_metrics_ans.test_metrics[\"recall\"]\n",
    "test_precision_ans = rf_metrics_ans.test_metrics[\"precision\"]\n",
    "test_fscore_ans = rf_metrics_ans.test_metrics[\"fscore\"]\n",
    "test_fpr_ans = rf_metrics_ans.test_metrics[\"fpr\"]\n",
    "test_fnr_ans = rf_metrics_ans.test_metrics[\"fnr\"]\n",
    "test_roc_auc_ans = rf_metrics_ans.test_metrics[\"roc_auc\"]\n",
    "\n",
    "# Calculate Metrics with Student's Function\n",
    "rf_metrics,_ = calculate_random_forest_metrics(train_features, test_features, train_targets, test_targets, rf_kwargs)\n",
    "train_accuracy = rf_metrics.train_metrics[\"accuracy\"]\n",
    "assert train_accuracy == train_accuracy_ans\n",
    "train_recall = rf_metrics.train_metrics[\"recall\"]\n",
    "assert train_recall == train_recall_ans\n",
    "train_precision = rf_metrics.train_metrics[\"precision\"]\n",
    "assert train_precision == train_precision_ans\n",
    "train_fscore = rf_metrics.train_metrics[\"fscore\"]\n",
    "assert train_fscore == train_fscore_ans\n",
    "train_fpr = rf_metrics.train_metrics[\"fpr\"]\n",
    "assert train_fpr == train_fpr_ans\n",
    "train_fnr = rf_metrics.train_metrics[\"fnr\"]\n",
    "assert train_fnr == train_fnr_ans\n",
    "train_roc_auc = rf_metrics.train_metrics[\"roc_auc\"]\n",
    "assert train_roc_auc == train_roc_auc_ans\n",
    "test_accuracy = rf_metrics.test_metrics[\"accuracy\"]\n",
    "assert test_accuracy == test_accuracy_ans\n",
    "test_recall = rf_metrics.test_metrics[\"recall\"]\n",
    "assert test_recall == test_recall_ans\n",
    "test_precision = rf_metrics.test_metrics[\"precision\"]\n",
    "assert test_precision == test_precision_ans\n",
    "test_fscore = rf_metrics.test_metrics[\"fscore\"]\n",
    "assert test_fscore == test_fscore_ans\n",
    "test_fpr = rf_metrics.test_metrics[\"fpr\"]\n",
    "assert test_fpr == test_fpr_ans\n",
    "test_fnr = rf_metrics.test_metrics[\"fnr\"]\n",
    "assert test_fnr == test_fnr_ans\n",
    "test_roc_auc = rf_metrics.test_metrics[\"roc_auc\"]\n",
    "assert test_roc_auc == test_roc_auc_ans\n",
    "importance_df = rf_metrics.feat_imp_df\n",
    "if compare_submission_to_answer_df(importance_df.round(4),rf_importance_df_ans,\"RF Feature Importance DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcdfc6",
   "metadata": {},
   "source": [
    "## `calculate_gradient_boosting_metrics`\n",
    "\n",
    "A Gradient Boosted model is a more complex model than the Naive and Logistic Regression Models and similar in structure to the Random Forest Model you just trained. A Gradient Boosted Model Expands on the Tree Based Model by using its additional trees to predict the errors from the previous tree. For this project, we are more focused on showing you how to apply these models, so you can just use the sklearn Gradient Boosted Model in your code. \n",
    "\n",
    "For this task use Sklearn's Gradient Boosting Classifier class and complete the following subtasks:\n",
    "* Train a Gradient Boosted model (initialized using the kwargs passed into the function)\n",
    "* Predict scores for training and test datasets and calculate the 7 metrics listed below for the training and test datasets using predictions from the fit model. (All rounded to 4 decimal places)\n",
    "  * `accuracy`\n",
    "  * `recall`\n",
    "  * `precision`\n",
    "  * `fscore`\n",
    "  * `false positive rate (fpr)`\n",
    "  * `false negative rate (fnr)`\n",
    "  * `Area Under the Curve of Receiver Operating Characteristics Curve (roc_auc)`\n",
    "* Create a Feature Importance DataFrame from the trained model: \n",
    "  * **Do Not Use RFE for feature selection**\n",
    "  * Use the top 10 features selected by the built in method (sorted from biggest to smallest) \n",
    "  * Make sure you use the same feature and importance column names as ModelMetrics shows in feat_name_col [`Feature`] and imp_col [`Importance`]\n",
    "  * round the importances to 4 decimal places \n",
    "  * Reset the index to 0-9 you can do this the same way you did in task1 \n",
    "\n",
    "**NOTE:** Make sure you use the predicted probabilities for roc auc\n",
    "\n",
    "##### Useful Resources\n",
    "* https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "* https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "\n",
    "##### INPUTS\n",
    "* `train_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_features` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `train_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `test_targets` - a dataset split by a function similar to the tts function you created in task2\n",
    "* `gb_kwargs` - a dictionary with keyword arguments that can be passed directly to the sklearn GradientBoostingClassifier class\n",
    "\n",
    "##### OUTPUTS \n",
    "* A completed `ModelMetrics` object with a training and test metrics dictionary with each one of the metrics **rounded to 4 decimal places**\n",
    "* An sklearn Gradient Boosted model object fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_gradient_boosting_metrics(train_features:pd.DataFrame, test_features:pd.DataFrame, train_targets:pd.Series, test_targets:pd.Series, gb_kwargs) -> tuple[ModelMetrics,GradientBoostingClassifier]:\n",
    "    # TODO: Write the necessary code to train a gradient boosting binary classification model and calculate accuracy, recall, precision, fscore, \n",
    "    # false positive rate, false negative rate and area under the reciever operator curve given a train and test dataframe and train and test \n",
    "    # target series and keyword arguments for the gradient boosting model\n",
    "\n",
    "    model = GradientBoostingClassifier()\n",
    "    train_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "    test_metrics = {\n",
    "        \"accuracy\" : 0,\n",
    "        \"recall\" : 0,\n",
    "        \"precision\" : 0,\n",
    "        \"fscore\" : 0,\n",
    "        \"fpr\" : 0,\n",
    "        \"fnr\" : 0,\n",
    "        \"roc_auc\" : 0\n",
    "        }\n",
    "\n",
    "    gb_importance = pd.DataFrame()\n",
    "    gb_metrics = ModelMetrics(\"Gradient Boosting\",train_metrics,test_metrics,gb_importance)\n",
    "\n",
    "    return gb_metrics,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29abbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your code\n",
    "gb_kwargs = {'n_estimators':150,'criterion':'squared_error','max_depth':3,'random_state':0}\n",
    "# Answers for Wine Dataset\n",
    "with open(os.path.join(os.getcwd(),\"..\",\"task4\",\"pkl_files\",\"gb_metrics.pkl\"), 'rb') as file:\n",
    "    gb_metrics_ans = pickle.load(file)\n",
    "train_accuracy_ans = gb_metrics_ans.train_metrics[\"accuracy\"]\n",
    "train_recall_ans = gb_metrics_ans.train_metrics[\"recall\"]\n",
    "train_precision_ans = gb_metrics_ans.train_metrics[\"precision\"]\n",
    "train_fscore_ans = gb_metrics_ans.train_metrics[\"fscore\"]\n",
    "train_fpr_ans = gb_metrics_ans.train_metrics[\"fpr\"]\n",
    "train_fnr_ans = gb_metrics_ans.train_metrics[\"fnr\"]\n",
    "train_roc_auc_ans = gb_metrics_ans.train_metrics[\"roc_auc\"]\n",
    "\n",
    "test_accuracy_ans = gb_metrics_ans.test_metrics[\"accuracy\"]\n",
    "test_recall_ans = gb_metrics_ans.test_metrics[\"recall\"]\n",
    "test_precision_ans = gb_metrics_ans.test_metrics[\"precision\"]\n",
    "test_fscore_ans = gb_metrics_ans.test_metrics[\"fscore\"]\n",
    "test_fpr_ans = gb_metrics_ans.test_metrics[\"fpr\"]\n",
    "test_fnr_ans = gb_metrics_ans.test_metrics[\"fnr\"]\n",
    "test_roc_auc_ans = gb_metrics_ans.test_metrics[\"roc_auc\"]\n",
    "\n",
    "# Calculate Metrics with Student's Function\n",
    "gb_metrics,_ = calculate_gradient_boosting_metrics(train_features,test_features,train_targets,test_targets,gb_kwargs)\n",
    "\n",
    "train_accuracy = gb_metrics.train_metrics[\"accuracy\"]\n",
    "assert train_accuracy == train_accuracy_ans\n",
    "train_recall = gb_metrics.train_metrics[\"recall\"]\n",
    "assert train_recall == train_recall_ans\n",
    "train_precision = gb_metrics.train_metrics[\"precision\"]\n",
    "assert train_precision == train_precision_ans\n",
    "train_fscore = gb_metrics.train_metrics[\"fscore\"]\n",
    "assert train_fscore == train_fscore_ans\n",
    "train_fpr = gb_metrics.train_metrics[\"fpr\"]\n",
    "assert train_fpr == train_fpr_ans\n",
    "train_fnr = gb_metrics.train_metrics[\"fnr\"]\n",
    "assert train_fnr == train_fnr_ans\n",
    "train_roc_auc = gb_metrics.train_metrics[\"roc_auc\"]\n",
    "assert train_roc_auc == train_roc_auc_ans\n",
    "test_accuracy = gb_metrics.test_metrics[\"accuracy\"]\n",
    "assert test_accuracy == test_accuracy_ans\n",
    "test_recall = gb_metrics.test_metrics[\"recall\"]\n",
    "assert test_recall == test_recall_ans\n",
    "test_precision = gb_metrics.test_metrics[\"precision\"]\n",
    "assert test_precision == test_precision_ans\n",
    "test_fscore = gb_metrics.test_metrics[\"fscore\"]\n",
    "assert test_fscore == test_fscore_ans\n",
    "test_fpr = gb_metrics.test_metrics[\"fpr\"]\n",
    "assert test_fpr == test_fpr_ans\n",
    "test_fnr = gb_metrics.test_metrics[\"fnr\"]\n",
    "assert test_fnr == test_fnr_ans\n",
    "test_roc_auc = gb_metrics.test_metrics[\"roc_auc\"]\n",
    "print(test_roc_auc)\n",
    "assert test_roc_auc == test_roc_auc_ans\n",
    "importance_df = gb_metrics.feat_imp_df\n",
    "if compare_submission_to_answer_df(importance_df.round(4),gb_importance_df_ans,\"GB Feature Importance DF\"):\n",
    "    print('Passed...')\n",
    "else:\n",
    "    print('Failed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d9426",
   "metadata": {},
   "source": [
    "# You have successfully reached the end of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
